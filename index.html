<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Digital Human Interface</title>
    <!-- Three.js åº“ (Three.js Library) -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/0.128.0/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/loaders/GLTFLoader.js"></script>
    <style>
        /* --- æ•´ä½“é¡µé¢ä¸å­—ä½“å®šä¹‰ (Overall Page & Font Definition) --- */
        body, html {
            height: 100%; /* ç¡®ä¿htmlå’Œbodyå æ®å…¨éƒ¨è§†å£é«˜åº¦ (Ensure html and body take full viewport height) */
            margin: 0;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; /* æ›´ç°ä»£çš„æ— è¡¬çº¿å­—ä½“ (More modern sans-serif font) */
            background-color: #f4f7f6; /* æ•´ä½“é¡µé¢èƒŒæ™¯è‰² (Overall page background color) */
            color: #333; /* é»˜è®¤æ–‡å­—é¢œè‰² (Default text color) */
            font-size: 16px; /* åŸºå‡†å­—ä½“å¤§å° (Base font size) */
        }

        /* --- ä¸»å®¹å™¨Flexå¸ƒå±€ (Main Container Flex Layout) --- */
        .main-container {
            display: flex;
            height: 100vh; /* ä½¿ç”¨è§†å£é«˜åº¦å•ä½ (Use viewport height unit) */
            width: 100%;
        }

        /* --- æ•°å­—äººå®¹å™¨ (Digital Human Container) --- */
        #digital-human-container {
            width: 50%;
            height: 100%;
            background-color: #1e1e1e; /* è¾ƒæš—çš„èƒŒæ™¯ä»¥çªå‡º3Dæ¨¡å‹ (Darker background for 3D model prominence) */
            border-right: 1px solid #d0d0d0; /* å®¹å™¨é—´çš„ç»†åˆ†éš”çº¿ (Subtle separator line) */
            box-sizing: border-box;
            position: relative; 
            overflow: hidden; 
        }
        /* ç§»é™¤é»˜è®¤çš„ <p>Digital Human Display Area</p> æ ‡ç­¾çš„æ˜¾ç¤º (Remove display of default p tag) */
        #digital-human-container > p {
            display: none;
        }


        /* --- èŠå¤©å®¹å™¨ (Chat Container) --- */
        #chat-container {
            width: 50%;
            height: 100%;
            background-color: #ffffff; /* èŠå¤©åŒºåŸŸä½¿ç”¨ç™½è‰²èƒŒæ™¯ (White background for chat area) */
            display: flex;
            flex-direction: column;
            justify-content: space-between;
            box-sizing: border-box;
            padding: 0; /* ç§»é™¤å†…è¾¹è·ï¼Œç”±å­å…ƒç´ æ§åˆ¶ (Remove padding, controlled by children) */
        }

        /* --- èŠå¤©æ¶ˆæ¯æ˜¾ç¤ºåŒºåŸŸ (Chat Messages Display Area) --- */
        .chat-messages {
            flex-grow: 1;
            padding: 20px; /* å¢åŠ å†…è¾¹è· (Increased padding) */
            overflow-y: auto; /* å…è®¸å†…å®¹æ»šåŠ¨ (Allow content to scroll) */
            display: flex; /* ä½¿ç”¨flexå¸ƒå±€æ¥ç®¡ç†æ¶ˆæ¯å¯¹é½ (Use flex for message alignment) */
            flex-direction: column; /* æ¶ˆæ¯å‚ç›´æ’åˆ— (Messages stack vertically) */
            gap: 12px; /* æ¶ˆæ¯é—´çš„é—´éš™ (Gap between messages) */
        }
        /* ç§»é™¤åˆå§‹çš„ <p>Welcome...</p> æ ‡ç­¾çš„é»˜è®¤è¾¹è·å’Œç‰¹å®šæ ·å¼ (Remove default margin and style for initial p tag) */
        .chat-messages > p:first-child {
            margin: 0 auto; /* å±…ä¸­å¹¶ç§»é™¤ä¸Šä¸‹è¾¹è· (Center and remove top/bottom margin) */
            padding: 10px 0; /* æ·»åŠ ä¸€äº›ä¸Šä¸‹å†…è¾¹è· (Add some top/bottom padding) */
            color: #888; /* æŸ”å’Œçš„æ–‡å­—é¢œè‰² (Softer text color) */
            font-style: italic;
            text-align: center;
            font-size: 0.9em; /* ç¨å°å­—ä½“ (Slightly smaller font) */
        }


        /* --- å•æ¡æ¶ˆæ¯æ ·å¼ (Individual Message Styling) --- */
        .message { /* é€šç”¨æ¶ˆæ¯æ ·å¼ (Common message style) */
            padding: 10px 15px; /* æ¶ˆæ¯å†…è¾¹è· (Padding within messages) */
            border-radius: 18px; /* æ›´åœ†æ¶¦çš„è¾¹è§’ (Softer rounded corners) */
            max-width: 75%; /* æ¶ˆæ¯æœ€å¤§å®½åº¦ (Max width for messages) */
            line-height: 1.5; /* æ”¹å–„å¯è¯»æ€§ (Improved readability) */
            word-wrap: break-word; /* ç¡®ä¿é•¿å•è¯æ¢è¡Œ (Ensure long words wrap) */
            box-shadow: 0 2px 4px rgba(0,0,0,0.07); /* è½»å¾®é˜´å½± (Subtle shadow) */
        }

        .user-message { /* ç”¨æˆ·æ¶ˆæ¯ç‰¹å®šæ ·å¼ (User message specific style) */
            background-color: #007bff; /* é²œæ˜çš„ç”¨æˆ·æ¶ˆæ¯é¢œè‰² (Distinct user message color) */
            color: white;
            align-self: flex-end; /* ç”¨æˆ·æ¶ˆæ¯é å³ (User messages align to the right) */
            margin-left: auto; 
            border-bottom-right-radius: 5px; /* è°ƒæ•´ç‰¹å®šè¾¹è§’ä»¥å½¢æˆå¯¹è¯æ°”æ³¡æ„Ÿ (Adjust specific corner for bubble feel) */
        }

        .dify-message { /* Dify(AI)æ¶ˆæ¯ç‰¹å®šæ ·å¼ (Dify(AI) message specific style) */
            background-color: #e9ecef; /* AIæ¶ˆæ¯ä½¿ç”¨æµ…ç°è‰² (Light grey for AI messages) */
            color: #212529; /* æ·±ç°è‰²æ–‡å­—ä»¥ä¿è¯å¯¹æ¯”åº¦ (Darker grey text for contrast) */
            align-self: flex-start; /* AIæ¶ˆæ¯é å·¦ (AI messages align to the left) */
            margin-right: auto;
            border-bottom-left-radius: 5px; /* è°ƒæ•´ç‰¹å®šè¾¹è§’ (Adjust specific corner) */
        }
        
        /* --- èŠå¤©è¾“å…¥åŒºåŸŸ (Chat Input Area) --- */
        .chat-input {
            padding: 15px 20px; /* è¾“å…¥åŒºåŸŸå†…è¾¹è· (Padding for input area) */
            display: flex;
            align-items: center; /* å‚ç›´å±…ä¸­å¯¹é½è¾“å…¥æ¡†å’ŒæŒ‰é’® (Vertically align input and buttons) */
            border-top: 1px solid #e0e0e0; /* è¾“å…¥åŒºåŸŸé¡¶éƒ¨åˆ†éš”çº¿ (Separator line above input area) */
            background-color: #f8f9fa; /* è¾“å…¥åŒºåŸŸèƒŒæ™¯è‰² (Background color for input area) */
        }

        .chat-input input[type="text"] {
            flex-grow: 1;
            padding: 12px 18px; /* è¾“å…¥æ¡†å†…è¾¹è· (Padding within input field) */
            border: 1px solid #ced4da; /* è¾¹æ¡†é¢œè‰² (Border color) */
            border-radius: 22px; /* æ›´åœ†æ¶¦çš„è¾“å…¥æ¡† (More rounded input field) */
            font-size: 1rem; /* å­—ä½“å¤§å° (Font size) */
            margin-right: 10px; /* ä¸æŒ‰é’®çš„é—´è· (Spacing from buttons) */
            outline: none; /* ç§»é™¤é»˜è®¤ç„¦ç‚¹è½®å»“ (Remove default focus outline) */
            transition: border-color 0.2s ease, box-shadow 0.2s ease; /* å¹³æ»‘è¿‡æ¸¡æ•ˆæœ (Smooth transition effect) */
        }
        .chat-input input[type="text"]:focus {
            border-color: #80bdff; /* ç„¦ç‚¹æ—¶è¾¹æ¡†é¢œè‰² (Border color on focus) */
            box-shadow: 0 0 0 0.2rem rgba(0,123,255,.25); /* ç„¦ç‚¹æ—¶é˜´å½± (Shadow on focus) */
        }

        .chat-input button {
            padding: 10px 18px; /* æŒ‰é’®å†…è¾¹è· (Button padding) */
            border: none;
            border-radius: 22px; /* åœ†æ¶¦æŒ‰é’® (Rounded buttons) */
            background-color: #007bff; /* æŒ‰é’®èƒŒæ™¯è‰² (Button background color) */
            color: white;
            font-size: 1rem; /* å­—ä½“å¤§å° (Font size) */
            cursor: pointer;
            transition: background-color 0.2s ease; /* å¹³æ»‘è¿‡æ¸¡æ•ˆæœ (Smooth transition effect) */
            display: flex; /* ç”¨äºå±…ä¸­å›¾æ ‡å’Œæ–‡æœ¬ï¼ˆå¦‚æœæœªæ¥æœ‰ï¼‰(For centering icon and text if any in future) */
            align-items: center;
            justify-content: center;
        }
        .chat-input button#mic-button { /* éº¦å…‹é£æŒ‰é’®ç‰¹å®šæ ·å¼ (Mic button specific style) */
            background-color: #6c757d; /* éº¦å…‹é£æŒ‰é’®ä½¿ç”¨ä¸åŒé¢œè‰² (Different color for mic button) */
            margin-left: 8px; /* è°ƒæ•´ä¸å‘é€æŒ‰é’®çš„é—´è· (Adjust spacing from send button) */
            padding: 10px 12px; /* è°ƒæ•´éº¦å…‹é£æŒ‰é’®çš„ padding ä½¿å…¶æ›´åƒåœ†å½¢ (Adjust mic button padding to be more circular) */
        }
        .chat-input button#mic-button:hover {
            background-color: #5a6268; /* éº¦å…‹é£æŒ‰é’®æ‚¬åœé¢œè‰² (Mic button hover color) */
        }
        .chat-input button#mic-button.recording { /* å½•éŸ³çŠ¶æ€ä¸‹çš„éº¦å…‹é£æŒ‰é’® (Mic button when recording) */
            background-color: #dc3545; /* çº¢è‰²è¡¨ç¤ºå½•éŸ³ä¸­ (Red to indicate recording) */
            box-shadow: 0 0 8px rgba(220, 53, 69, 0.7); /* å½•éŸ³æ—¶å‘å…‰æ•ˆæœ (Glowing effect when recording) */
        }

        .chat-input button:hover {
            background-color: #0056b3; /* æŒ‰é’®æ‚¬åœèƒŒæ™¯è‰² (Button hover background color) */
        }
        
        /* --- æ»šåŠ¨æ¡ç¾åŒ– (Scrollbar Prettification - WebKit Browsers) --- */
        .chat-messages::-webkit-scrollbar {
            width: 8px; /* æ»šåŠ¨æ¡å®½åº¦ (Scrollbar width) */
        }
        .chat-messages::-webkit-scrollbar-track {
            background: #f8f9fa; /* æ»šåŠ¨æ¡è½¨é“èƒŒæ™¯ (Scrollbar track background) */
            border-radius: 10px; /* è½¨é“åœ†è§’ (Track border radius) */
        }
        .chat-messages::-webkit-scrollbar-thumb {
            background: #ced4da; /* æ»šåŠ¨æ¡æ»‘å—é¢œè‰² (Scrollbar thumb color) */
            border-radius: 10px; /* æ»‘å—åœ†è§’ (Thumb border radius) */
        }
        .chat-messages::-webkit-scrollbar-thumb:hover {
            background: #adb5bd; /* æ»‘å—æ‚¬åœé¢œè‰² (Thumb hover color) */
        }

    </style>
</head>
<body>
    <div class="main-container">
        <div id="digital-human-container">
            <!-- <p>Digital Human Display Area</p> --> <!-- This p tag is hidden by CSS -->
        </div>
        <div id="chat-container">
            <div class="chat-messages" id="chat-messages-display">
                <!-- Chat messages will appear here -->
                <p>Welcome! Chat with the digital human.</p>
            </div>
            <div class="chat-input">
                <input type="text" id="user-input" placeholder="Type your message...">
                <button id="send-button">Send</button>
                <button id="mic-button">ğŸ¤</button> <!-- éº¦å…‹é£æŒ‰é’®, ç§»é™¤äº†å†…è”æ ·å¼ (Mic button, removed inline style) -->
            </div>
        </div>
    </div>

    <script>
        // JavaScriptä»£ç å°†æ”¾åœ¨è¿™é‡Œ
        // Dify Chatç›¸å…³çš„DOMå…ƒç´ å’Œå¸¸é‡ (Dify Chat related DOM elements and constants)
        const chatMessagesDisplay = document.getElementById('chat-messages-display'); // è·å–èŠå¤©æ¶ˆæ¯æ˜¾ç¤ºåŒºåŸŸçš„div
        const userInput = document.getElementById('user-input'); // è·å–ç”¨æˆ·è¾“å…¥æ¡†
        const sendButton = document.getElementById('send-button'); // è·å–å‘é€æŒ‰é’®
        const difyApiKey = 'app-Gs2XACPpISYGBMlyztY7dzGX'; // Dify APIå¯†é’¥
        const difyBaseUrl = 'http://172.18.152.20/v1/chat-messages'; // Dify APIåŸºç¡€URL
        const userId = 'web-user'; // é™æ€ç”¨æˆ·ID

        // STT (Speech-to-Text) ç›¸å…³çš„DOMå…ƒç´ å’Œå¸¸é‡ (STT related DOM elements and constants)
        const micButton = document.getElementById('mic-button'); // è·å–éº¦å…‹é£æŒ‰é’®
        const STT_WEBSOCKET_URL = 'ws://localhost:8765/stt'; // ESPnet STT WebSocketæœåŠ¡URL (å¯é…ç½®) (ESPnet STT WebSocket service URL, configurable)
        
        // TTS (Text-to-Speech) å¸¸é‡ (TTS constants)
        const TTS_HTTP_URL = 'http://localhost:8766/tts'; // ESPnet TTS HTTPæœåŠ¡URL (å¯é…ç½®) (ESPnet TTS HTTP service URL, configurable)

        // STT/TTS å…±äº«çš„ AudioContext å’Œåˆ†æå™¨ (Shared AudioContext and Analyser for STT/TTS)
        let audioContext; // AudioContextå®ä¾‹ (AudioContext instance) - Shared for STT and TTS audio analysis
        let ttsAnalyser = null; // TTSç”¨çš„AnalyserNode (AnalyserNode for TTS)

        // STT çŠ¶æ€å˜é‡ (STT state variables)
        let sttWs; // WebSocketå®ä¾‹ (WebSocket instance)
        let mediaStream; // MediaStreamå®ä¾‹ (MediaStream instance from getUserMedia)
        let scriptProcessor; // ScriptProcessorNodeå®ä¾‹ (ScriptProcessorNode instance)
        let isRecording = false; // å½•éŸ³çŠ¶æ€ (Recording state)
        const TARGET_SAMPLE_RATE = 16000; // STTæœåŠ¡æœŸæœ›çš„é‡‡æ ·ç‡ (Target sample rate for STT service)
        
        // TTS çŠ¶æ€å˜é‡ (TTS state variables)
        let currentTTSAudio = null; // å½“å‰æ­£åœ¨æ’­æ”¾çš„TTSéŸ³é¢‘å¯¹è±¡ (Current playing TTS audio object)
        let ttsAudioSourceNode = null; // TTS éŸ³é¢‘æºèŠ‚ç‚¹ (TTS audio source node for analyser)


        // æ·»åŠ æ¶ˆæ¯åˆ°èŠå¤©æ˜¾ç¤ºåŒºåŸŸçš„è¾…åŠ©å‡½æ•°
        // å‚æ•°: message (string) - è¦æ˜¾ç¤ºçš„æ¶ˆæ¯å†…å®¹ (Parameter: message (string) - message content to display)
        // å‚æ•°: sender (string) - å‘é€è€… ('user' æˆ– 'dify') (Parameter: sender (string) - sender ('user' or 'dify'))
        function addMessageToChat(message, sender) {
            const messageElement = document.createElement('div'); // åˆ›å»ºä¸€ä¸ªæ–°çš„divå…ƒç´ æ¥æ˜¾ç¤ºæ¶ˆæ¯ (Create a new div element to display the message)
            messageElement.classList.add('message'); // æ·»åŠ  'message' ç±» (Add 'message' class)
            messageElement.classList.add(sender === 'user' ? 'user-message' : 'dify-message'); // æ ¹æ®å‘é€è€…æ·»åŠ ç‰¹å®šçš„ç±» (Add specific class based on sender)
            
            const messageTextNode = document.createTextNode(message); // åˆ›å»ºæ–‡æœ¬èŠ‚ç‚¹ (Create a text node)
            messageElement.appendChild(messageTextNode); // å°†æ–‡æœ¬èŠ‚ç‚¹æ·»åŠ åˆ°æ¶ˆæ¯å…ƒç´ ä¸­ (Append text node to message element)

            chatMessagesDisplay.appendChild(messageElement); // å°†æ¶ˆæ¯å…ƒç´ æ·»åŠ åˆ°èŠå¤©æ˜¾ç¤ºåŒºåŸŸ (Append message element to chat display area)
            chatMessagesDisplay.scrollTop = chatMessagesDisplay.scrollHeight; // æ»šåŠ¨åˆ°åº•éƒ¨ä»¥æ˜¾ç¤ºæœ€æ–°æ¶ˆæ¯ (Scroll to bottom to show latest message)
        }

        // --- TTS (Text-to-Speech) åŠŸèƒ½ ---
        // (TTS Functionality)

        // å°†æ–‡æœ¬ä½œä¸ºè¯­éŸ³æ’­æ”¾åŠŸèƒ½ (Function to play text as speech)
        // å‚æ•°: text (string) - éœ€è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬ (Parameter: text (string) - text to be converted to speech)
        async function playTextAsSpeech(text) {
            if (!text || text.trim() === "") { // å¦‚æœæ–‡æœ¬ä¸ºç©ºæˆ–ä»…åŒ…å«ç©ºæ ¼ï¼Œåˆ™ä¸æ‰§è¡Œä»»ä½•æ“ä½œ (If text is empty or only whitespace, do nothing)
                console.log("TTS: æ–‡æœ¬ä¸ºç©ºï¼Œä¸è¿›è¡Œæ’­æ”¾ã€‚"); // TTS: Text is empty, skipping playback.
                return;
            }

            if (currentTTSAudio && !currentTTSAudio.paused) { // å¦‚æœå½“å‰æœ‰éŸ³é¢‘æ­£åœ¨æ’­æ”¾ (If there's currently playing audio)
                currentTTSAudio.pause(); // æš‚åœå½“å‰éŸ³é¢‘ (Pause current audio)
                console.log("TTS: å·²æš‚åœä¸Šä¸€ä¸ªéŸ³é¢‘ã€‚"); // TTS: Paused previous audio.
                if (ttsAudioSourceNode) {
                    ttsAudioSourceNode.disconnect(); // æ–­å¼€æ—§çš„éŸ³é¢‘æº (Disconnect old audio source)
                    ttsAudioSourceNode = null;
                }
                if (ttsAnalyser) {
                    // ttsAnalyser.disconnect(); // åˆ†æå™¨é€šå¸¸è¿æ¥åˆ°destinationï¼Œé™¤éè¦å®Œå…¨é‡ç½® (Analyser usually connected to destination, unless full reset)
                    ttsAnalyser = null; // é‡ç½®åˆ†æå™¨å¼•ç”¨ï¼Œä»¥ä¾¿åŠ¨ç”»å¾ªç¯çŸ¥é“åœæ­¢ (Reset analyser reference so animation loop knows to stop)
                }
            }

            const payload = { text: text }; // å‡†å¤‡è¯·æ±‚ä½“ (Prepare request body)
            console.log("TTS: å‘é€è¯·æ±‚åˆ°", TTS_HTTP_URL, "å†…å®¹:", payload); // TTS: Sending request to URL with payload

            try {
                const response = await fetch(TTS_HTTP_URL, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify(payload)
                });

                if (response.ok) {
                    const audioBlob = await response.blob(); // è·å–éŸ³é¢‘æ•°æ®ä¸ºBlob (Get audio data as Blob)
                    console.log("TTS: æ”¶åˆ°éŸ³é¢‘Blob, ç±»å‹:", audioBlob.type); // TTS: Received audio Blob, type:
                    const audioUrl = URL.createObjectURL(audioBlob); // åˆ›å»ºå¯¹è±¡URL (Create object URL)
                    
                    currentTTSAudio = new Audio(audioUrl); // åˆ›å»ºæ–°çš„Audioå¯¹è±¡ (Create new Audio object)
                    
                    // åˆå§‹åŒ–æˆ–å¤ç”¨AudioContext (Initialize or reuse AudioContext)
                    if (!audioContext || audioContext.state === 'closed') {
                        audioContext = new (window.AudioContext || window.webkitAudioContext)();
                        console.log("TTS: æ–°å»ºæˆ–é‡æ–°åˆ›å»ºäº†AudioContextã€‚"); // TTS: New or recreated AudioContext.
                    } else {
                        console.log("TTS: å¤ç”¨å·²æœ‰çš„AudioContextã€‚"); // TTS: Reusing existing AudioContext.
                    }

                    // åˆ›å»ºéŸ³é¢‘åˆ†æè®¾ç½® (Create audio analysis setup)
                    ttsAudioSourceNode = audioContext.createMediaElementSource(currentTTSAudio); // ä»audioå…ƒç´ åˆ›å»ºæº (Create source from audio element)
                    ttsAnalyser = audioContext.createAnalyser(); // åˆ›å»ºåˆ†æå™¨ (Create analyser)
                    ttsAnalyser.fftSize = 512; // è®¾ç½®FFTå¤§å° (Set FFT size)

                    // è¿æ¥éŸ³é¢‘å›¾: æº -> åˆ†æå™¨ -> è¾“å‡º(æ‰¬å£°å™¨) (Connect audio graph: source -> analyser -> destination)
                    ttsAudioSourceNode.connect(ttsAnalyser);
                    ttsAnalyser.connect(audioContext.destination);
                    
                    currentTTSAudio.play().catch(e => console.error("TTS: æ’­æ”¾éŸ³é¢‘å¤±è´¥:", e)); // æ’­æ”¾éŸ³é¢‘å¹¶æ•è·å¯èƒ½çš„é”™è¯¯ (Play audio and catch potential errors)
                    console.log("TTS: å¼€å§‹æ’­æ”¾éŸ³é¢‘å¹¶è®¾ç½®åˆ†æå™¨ã€‚"); // TTS: Starting audio playback and setting up analyser.

                    currentTTSAudio.onended = () => {
                        console.log("TTS: éŸ³é¢‘æ’­æ”¾å®Œæ¯•ã€‚"); // TTS: Audio playback finished.
                        URL.revokeObjectURL(audioUrl); // é‡Šæ”¾å¯¹è±¡URLèµ„æº (Release object URL resources)
                        currentTTSAudio = null; // æ¸…ç†å½“å‰éŸ³é¢‘å¯¹è±¡å¼•ç”¨ (Clear current audio object reference)
                        if (ttsAudioSourceNode) {
                            ttsAudioSourceNode.disconnect(); // æ–­å¼€éŸ³é¢‘æº (Disconnect audio source)
                            ttsAudioSourceNode = null;
                        }
                        // ttsAnalyser.disconnect(); // åˆ†æå™¨é€šå¸¸è¿æ¥åˆ°destinationï¼Œä¸éœ€è¦åœ¨è¿™é‡Œæ–­å¼€ï¼Œé™¤éAudioContextå…³é—­
                        ttsAnalyser = null; // æŒ‡ç¤ºåŠ¨ç”»å¾ªç¯åœæ­¢æ›´æ–°å£å‹ (Signal animation loop to stop updating lip sync)
                        // é‡ç½®å£å‹åˆ°é™æ­¢çŠ¶æ€ (Reset lip shape to resting state)
                        if (loadedGltfModel && mouthShapeIndex !== undefined && loadedGltfModel.morphTargetInfluences) {
                           loadedGltfModel.morphTargetInfluences[mouthShapeIndex] = 0;
                        }
                    };
                    currentTTSAudio.onerror = (e) => {
                        console.error("TTS: éŸ³é¢‘æ’­æ”¾é”™è¯¯ã€‚", e); // TTS: Audio playback error.
                        URL.revokeObjectURL(audioUrl); // å³ä½¿å‡ºé”™ä¹Ÿå°è¯•é‡Šæ”¾èµ„æº (Attempt to release resources even on error)
                        currentTTSAudio = null;
                        if (ttsAudioSourceNode) {
                            ttsAudioSourceNode.disconnect();
                            ttsAudioSourceNode = null;
                        }
                        ttsAnalyser = null;
                        alert("æ— æ³•æ’­æ”¾TTSéŸ³é¢‘ï¼Œè¯·æŸ¥çœ‹æ§åˆ¶å°ã€‚"); // Cannot play TTS audio, check console.
                    };
                } else {
                    // å¤„ç†TTS APIè¯·æ±‚é”™è¯¯ (Handle TTS API request error)
                    const errorText = await response.text();
                    console.error('TTS API Error:', response.status, errorText); // TTS APIé”™è¯¯ (TTS API Error)
                    alert(`TTSæœåŠ¡é”™è¯¯: ${response.status}. ${errorText}`); // TTS service error
                    ttsAnalyser = null; // ç¡®ä¿å‡ºé”™æ—¶ä¹Ÿæ¸…ç©ºåˆ†æå™¨å¼•ç”¨ (Ensure analyser ref is cleared on error)
                }
            } catch (error) {
                // æ•è·fetché”™è¯¯æˆ–å…¶ä»–ç½‘ç»œé”™è¯¯ (Capture fetch error or other network errors)
                console.error('TTS Fetch Error:', error); // TTS Fetché”™è¯¯ (TTS Fetch Error)
                alert('TTSè¯·æ±‚å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æœåŠ¡æ˜¯å¦å¯ç”¨ã€‚'); // TTS request failed, check network or service availability.
                ttsAnalyser = null; // ç¡®ä¿å‡ºé”™æ—¶ä¹Ÿæ¸…ç©ºåˆ†æå™¨å¼•ç”¨ (Ensure analyser ref is cleared on error)
            }
        }


        // å‘é€æ¶ˆæ¯åˆ°Dify APIçš„å‡½æ•° (Function to send message to Dify API)
        async function sendMessageToDify(messageText) {
            // å‡†å¤‡APIè¯·æ±‚çš„body (Prepare API request body)
            const requestBody = {
                "inputs": {}, // æ ¹æ®Dify APIæ–‡æ¡£ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦ç‰¹å®šçš„è¾“å…¥å­—æ®µ (According to Dify API docs, specific input fields might be needed here)
                "query": messageText, // ç”¨æˆ·çš„æ¶ˆæ¯æ–‡æœ¬ (User's message text)
                "user": userId, // ç”¨æˆ·çš„å”¯ä¸€æ ‡è¯†ç¬¦ (User's unique identifier)
                "response_mode": "blocking" // ä½¿ç”¨é˜»å¡æ¨¡å¼è·å–å“åº” (Use blocking mode to get response)
            };

            try {
                // å‘é€fetchè¯·æ±‚ (Send fetch request)
                const response = await fetch(difyBaseUrl, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${difyApiKey}`,
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify(requestBody)
                });

                if (response.ok) {
                    const data = await response.json(); // è§£æJSONå“åº” (Parse JSON response)
                    const difyReply = data.answer || (data.data && data.data.answer) || "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰å¾—åˆ°æ˜ç¡®çš„å›å¤ã€‚"; // ä»Difyå“åº”ä¸­æå–ç­”æ¡ˆï¼Œå¹¶æä¾›å¤‡é€‰æ–¹æ¡ˆ (Extract answer, provide fallback)
                    addMessageToChat(difyReply, 'dify'); // æ˜¾ç¤ºDifyçš„å›å¤ (Display Dify's reply)
                    playTextAsSpeech(difyReply); // è°ƒç”¨TTSåŠŸèƒ½æ’­æ”¾Difyçš„å›å¤ (Call TTS function to play Dify's reply)
                } else {
                    // å¤„ç†APIè¯·æ±‚é”™è¯¯ (Handle API request error)
                    const errorData = await response.text(); // è·å–æ–‡æœ¬å½¢å¼çš„é”™è¯¯ä¿¡æ¯ (Get error message as text)
                    console.error('Dify API Error:', response.status, errorData); // åœ¨æ§åˆ¶å°æ‰“å°é”™è¯¯ä¿¡æ¯ (Print error to console)
                    addMessageToChat(`Dify APIè¯·æ±‚é”™è¯¯: ${response.status}. ${errorData}`, 'dify'); // åœ¨èŠå¤©ç•Œé¢æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ (Display error in chat)
                }
            } catch (error) {
                // å¤„ç†ç½‘ç»œé”™è¯¯æˆ–å…¶ä»–fetché”™è¯¯ (Handle network error or other fetch errors)
                console.error('Fetch Error:', error); // åœ¨æ§åˆ¶å°æ‰“å°é”™è¯¯ä¿¡æ¯ (Print error to console)
                addMessageToChat('å‘é€æ¶ˆæ¯æ—¶é‡åˆ°ç½‘ç»œé”™è¯¯ã€‚', 'dify'); // åœ¨èŠå¤©ç•Œé¢æ˜¾ç¤ºç½‘ç»œé”™è¯¯ä¿¡æ¯ (Display network error in chat)
            }
        }

        // å‘é€æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ç›‘å¬å™¨ (Send button click event listener)
        sendButton.addEventListener('click', () => {
            const messageText = userInput.value.trim(); // è·å–ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯æ–‡æœ¬ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼ (Get user input, trim whitespace)
            if (messageText) { // å¦‚æœæ¶ˆæ¯ä¸ä¸ºç©º (If message is not empty)
                addMessageToChat(messageText, 'user'); // æ˜¾ç¤ºç”¨æˆ·å‘é€çš„æ¶ˆæ¯ (Display user's message)
                userInput.value = ''; // æ¸…ç©ºè¾“å…¥æ¡† (Clear input field)
                sendMessageToDify(messageText); // å°†æ¶ˆæ¯å‘é€åˆ°Dify API (Send message to Dify API)
            }
        });

        // ç”¨æˆ·è¾“å…¥æ¡†çš„é”®ç›˜äº‹ä»¶ç›‘å¬å™¨ (å¤„ç†Enteré”®) (User input keypress event listener for Enter key)
        userInput.addEventListener('keypress', (event) => {
            if (event.key === 'Enter') { // å¦‚æœæŒ‰ä¸‹çš„æ˜¯Enteré”® (If Enter key is pressed)
                event.preventDefault(); // é˜»æ­¢é»˜è®¤çš„Enteré”®è¡Œä¸º (ä¾‹å¦‚è¡¨å•æäº¤) (Prevent default Enter behavior e.g., form submission)
                sendButton.click(); // è§¦å‘å‘é€æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ (Trigger send button click)
            }
        });

        // --- STT (Speech-to-Text) åŠŸèƒ½å®ç° ---

        // å°†Float32Arrayè½¬æ¢ä¸ºInt16Array (PCM16) (Convert Float32Array to Int16Array (PCM16))
        // å‚æ•°: input (Float32Array) - è¾“å…¥çš„æµ®ç‚¹éŸ³é¢‘æ•°æ® (Input Float32 audio data)
        // è¿”å›: Int16Array - è½¬æ¢åçš„16ä½PCMæ•°æ® (Returned Int16 PCM data)
        function floatTo16BitPCM(input) {
            const output = new Int16Array(input.length);
            for (let i = 0; i < input.length; i++) {
                const s = Math.max(-1, Math.min(1, input[i]));
                output[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }
            return output;
        }

        // å¼€å§‹å½•éŸ³åŠŸèƒ½ (Start recording function)
        async function startRecording() {
            if (isRecording) return; // å¦‚æœå·²åœ¨å½•éŸ³ï¼Œåˆ™ä¸æ‰§è¡Œä»»ä½•æ“ä½œ (If already recording, do nothing)

            try {
                // 1. è¯·æ±‚éº¦å…‹é£æƒé™ (Request microphone permission)
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });

                // 2. åˆå§‹åŒ–WebSocketè¿æ¥ (Initialize WebSocket connection)
                sttWs = new WebSocket(STT_WEBSOCKET_URL);

                sttWs.onopen = () => {
                    console.log('STT WebSocket è¿æ¥æˆåŠŸã€‚'); // STT WebSocket connection successful.
                    // å‘é€å¼€å§‹ä¿¡å·ï¼ŒæŒ‡æ˜éŸ³é¢‘æ ¼å¼å’Œé‡‡æ ·ç‡ (Send start signal, specifying audio format and sample rate)
                    // æ³¨æ„: å®é™…åè®®å’Œé‡‡æ ·ç‡éœ€ä¸ESPnetæœåŠ¡å™¨é…ç½®ä¸€è‡´ (Note: Actual protocol and sample rate must match ESPnet server config)
                    // å‡è®¾æœåŠ¡å™¨æœŸæœ›æ¥æ”¶16kHzçš„PCM16æ•°æ® (Assuming server expects 16kHz PCM16 data)
                    sttWs.send(JSON.stringify({ 
                        action: 'start', 
                        'content-type': `audio/l16;rate=${TARGET_SAMPLE_RATE};channels=1` // æ˜ç¡®æŒ‡å®šå•å£°é“
                    }));
                    micButton.textContent = 'åœæ­¢'; // æ›´æ”¹æŒ‰é’®æ–‡æœ¬ä¸ºâ€œåœæ­¢â€ (Change button text to "Stop")
                    micButton.style.backgroundColor = 'red'; // æ›´æ”¹æŒ‰é’®é¢œè‰² (Change button color)
                    isRecording = true; // è®¾ç½®å½•éŸ³çŠ¶æ€ (Set recording state)
                };

                sttWs.onmessage = (event) => {
                    // console.log('STT WebSocket æ”¶åˆ°æ¶ˆæ¯:', event.data); // Log received message
                    try {
                        const data = JSON.parse(event.data);
                        if (data.transcript) {
                            if (data.final) {
                                userInput.value = data.transcript; // æœ€ç»ˆç»“æœï¼Œæ›¿æ¢è¾“å…¥æ¡†å†…å®¹ (Final result, replace input field content)
                                // å¯ä»¥é€‰æ‹©åœ¨è¿™é‡Œè‡ªåŠ¨å‘é€æ¶ˆæ¯
                                // sendButton.click(); 
                            } else {
                                // ä¸´æ—¶ç»“æœï¼Œå¯ä»¥å…ˆæ˜¾ç¤ºï¼Œæˆ–ç­‰æœ€ç»ˆç»“æœ (Interim result, can be displayed or wait for final)
                                // userInput.value = userInput.value.substring(0, userInput.selectionStart) + data.transcript + userInput.value.substring(userInput.selectionEnd);
                                userInput.value = data.transcript; // ç®€å•èµ·è§ï¼Œç›´æ¥æ›¿æ¢ (For simplicity, directly replace)
                            }
                        }
                        if (data.status === "ok" && data.message){ // ESPnet aSR server might send status messages
                            console.log("STT Server status: ", data.message);
                        }
                    } catch (e) {
                        console.error('è§£æSTTæ¶ˆæ¯å¤±è´¥:', e); // Failed to parse STT message
                        // userInput.value = "STT: " + event.data; // æ˜¾ç¤ºåŸå§‹æ•°æ®å¦‚æœä¸æ˜¯JSON (Display raw data if not JSON)
                    }
                };

                sttWs.onerror = (error) => {
                    console.error('STT WebSocket é”™è¯¯:', error); // STT WebSocket error
                    alert('STT WebSocket è¿æ¥é”™è¯¯ã€‚è¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œï¼Œæˆ–æŸ¥çœ‹æ§åˆ¶å°è·å–æ›´å¤šä¿¡æ¯ã€‚'); // STT WebSocket connection error. Check if service is running or see console.
                    stopRecording(); // å‘ç”Ÿé”™è¯¯æ—¶åœæ­¢å½•éŸ³ (Stop recording on error)
                };

                sttWs.onclose = (event) => {
                    console.log('STT WebSocket è¿æ¥å…³é—­ã€‚ä»£ç :', event.code, 'åŸå› :', event.reason); // STT WebSocket connection closed. Code, Reason
                    stopRecording(); // è¿æ¥å…³é—­æ—¶ç¡®ä¿åœæ­¢å½•éŸ³æµç¨‹ (Ensure recording process is stopped when connection closes)
                };

                // 3. è®¾ç½®éŸ³é¢‘å¤„ç† (Setup audio processing)
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                
                // æ£€æŸ¥å¹¶å¯èƒ½é‡é‡‡æ · (Check and potentially resample)
                // æ³¨æ„: æµè§ˆå™¨é€šå¸¸ä»¥44.1kHzæˆ–48kHzé‡‡æ ·ã€‚ESPneté€šå¸¸æœŸæœ›16kHzã€‚
                // å®¢æˆ·ç«¯é‡é‡‡æ ·ä¼šå¢åŠ å¤æ‚æ€§ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼ŒESPnetæœåŠ¡å™¨å¯ä»¥å¤„ç†è¾“å…¥é‡‡æ ·ç‡æˆ–é…ç½®ä¸ºæœŸæœ›çš„é‡‡æ ·ç‡ã€‚
                // æ­¤å¤„æˆ‘ä»¬å‡è®¾æœåŠ¡å™¨å¯ä»¥å¤„ç†AudioContextçš„åŸå§‹é‡‡æ ·ç‡ï¼Œæˆ–è€…æˆ‘ä»¬å·²åœ¨æœåŠ¡å™¨ç«¯é…ç½®äº†æ­£ç¡®çš„æœŸæœ›é‡‡æ ·ç‡ã€‚
                // å¦‚æœå¿…é¡»åœ¨å®¢æˆ·ç«¯é‡é‡‡æ ·ï¼Œéœ€è¦ä¸€ä¸ªé‡é‡‡æ ·åº“æˆ–æ‰‹åŠ¨å®ç°ã€‚
                // ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç°åœ¨å‘é€åŸå§‹æ•°æ®ï¼Œå¹¶åœ¨'start'ä¿¡å·ä¸­æŒ‡æ˜äº†TARGET_SAMPLE_RATEï¼Œä¾èµ–æœåŠ¡å™¨å¤„ç†ã€‚
                // ä½†æ›´å¥å£®çš„åšæ³•æ˜¯ç¡®ä¿å‘é€çš„æ•°æ®ç¡®å®æ˜¯TARGET_SAMPLE_RATEã€‚
                console.log(`AudioContext é‡‡æ ·ç‡: ${audioContext.sampleRate}, STTç›®æ ‡é‡‡æ ·ç‡: ${TARGET_SAMPLE_RATE}`);
                // if (audioContext.sampleRate !== TARGET_SAMPLE_RATE) {
                //     console.warn(`éŸ³é¢‘é‡‡æ ·ç‡ (${audioContext.sampleRate}Hz) ä¸ç›®æ ‡é‡‡æ ·ç‡ (${TARGET_SAMPLE_RATE}Hz) ä¸åŒ¹é…ã€‚ç†æƒ³æƒ…å†µä¸‹åº”è¿›è¡Œé‡é‡‡æ ·ã€‚`);
                //     // alert(`éŸ³é¢‘é‡‡æ ·ç‡ (${audioContext.sampleRate}Hz) ä¸ç›®æ ‡é‡‡æ ·ç‡ (${TARGET_SAMPLE_RATE}Hz) ä¸åŒ¹é…ã€‚è¯†åˆ«æ•ˆæœå¯èƒ½å—å½±å“ã€‚`);
                // }


                const bufferSize = 4096; // éŸ³é¢‘ç¼“å†²åŒºå¤§å° (Audio buffer size)
                const inputChannels = 1; // è¾“å…¥é€šé“æ•° (Number of input channels)
                const outputChannels = 1; // è¾“å‡ºé€šé“æ•° (Number of output channels)

                scriptProcessor = audioContext.createScriptProcessor(bufferSize, inputChannels, outputChannels);
                
                const source = audioContext.createMediaStreamSource(mediaStream);
                source.connect(scriptProcessor);
                scriptProcessor.connect(audioContext.destination); // éœ€è¦è¿æ¥åˆ°destinationï¼Œå³ä½¿ä¸æ’­æ”¾

                scriptProcessor.onaudioprocess = (event) => {
                    if (!isRecording || !sttWs || sttWs.readyState !== WebSocket.OPEN) {
                        return;
                    }
                    const inputData = event.inputBuffer.getChannelData(0); // è·å–å·¦å£°é“æ•°æ® (Get left channel data)
                    
                    // å°†Float32Arrayè½¬æ¢ä¸ºInt16Array (PCM)
                    const pcmData = floatTo16BitPCM(inputData);
                    sttWs.send(pcmData.buffer); // å‘é€PCMæ•°æ® (Send PCM data)
                };

            } catch (error) {
                console.error('å¯åŠ¨å½•éŸ³å¤±è´¥:', error); // Failed to start recording
                alert('æ— æ³•è®¿é—®éº¦å…‹é£ã€‚è¯·æ£€æŸ¥æƒé™ã€‚é”™è¯¯: ' + error.message); // Unable to access microphone. Check permissions.
                if (micButton) {
                    micButton.textContent = 'ğŸ¤'; // é‡ç½®æŒ‰é’®æ–‡æœ¬ (Reset button text)
                    micButton.style.backgroundColor = ''; // é‡ç½®æŒ‰é’®é¢œè‰² (Reset button color)
                }
                isRecording = false; // é‡ç½®å½•éŸ³çŠ¶æ€ (Reset recording state)
            }
        }

        // åœæ­¢å½•éŸ³åŠŸèƒ½ (Stop recording function)
        function stopRecording() {
            if (!isRecording && (!sttWs || sttWs.readyState === WebSocket.CLOSED) && !mediaStream) {
                // å¦‚æœä¸æ˜¯æ­£åœ¨å½•éŸ³ï¼Œä¸”WebSocketå·²å…³é—­æˆ–æœªåˆå§‹åŒ–ï¼Œä¸”mediaStreamå·²æ¸…ç†ï¼Œåˆ™é¿å…é‡å¤æ“ä½œ
                // console.log("å½•éŸ³å·²åœæ­¢æˆ–æœªåˆå§‹åŒ–ã€‚");
                if (micButton) {
                     micButton.textContent = 'ğŸ¤';
                     micButton.style.backgroundColor = '';
                }
                isRecording = false;
                return;
            }
            
            console.log("åœæ­¢å½•éŸ³..."); // Stopping recording...
            isRecording = false; // å…ˆè®¾ç½®çŠ¶æ€ï¼Œé¿å…onaudioprocessç»§ç»­å‘é€ (Set state first to prevent onaudioprocess from sending more)

            if (sttWs && sttWs.readyState === WebSocket.OPEN) {
                try {
                    sttWs.send(JSON.stringify({ action: 'stop' })); // å‘é€åœæ­¢ä¿¡å· (Send stop signal)
                    sttWs.close(1000, "å®¢æˆ·ç«¯åœæ­¢å½•éŸ³"); // æ­£å¸¸å…³é—­ (Normal closure)
                } catch (e) {
                    console.error("å‘é€åœæ­¢ä¿¡å·æˆ–å…³é—­WSæ—¶å‡ºé”™:", e);
                }
            }
            sttWs = null; // æ¸…ç†WebSocketå®ä¾‹ (Clean up WebSocket instance)

            if (scriptProcessor) {
                scriptProcessor.disconnect(); // æ–­å¼€ScriptProcessorNode (Disconnect ScriptProcessorNode)
                scriptProcessor.onaudioprocess = null; // ç§»é™¤äº‹ä»¶å¤„ç†å™¨ (Remove event handler)
                scriptProcessor = null;
            }
            // mediaStreamSource åœ¨ scriptProcessor æ–­å¼€åä¼šè‡ªåŠ¨å¤„ç†ï¼Œä½†æ˜ç¡®æ–­å¼€æ²¡æœ‰åå¤„
            // if (mediaStreamSource) { // mediaStreamSource å˜é‡åœ¨æ­¤ä»£ç ä¸­æœªå…¨å±€å®šä¹‰ï¼Œå®ƒæ˜¯åœ¨startRecordingä¸­ä½œä¸ºsourceä½¿ç”¨çš„
            //     // mediaStreamSource.disconnect(); 
            //     // mediaStreamSource = null;
            // }
             if (audioContext && audioContext.state !== 'closed') {
                audioContext.close().then(() => {
                    console.log("AudioContextå·²å…³é—­"); // AudioContext closed
                }).catch(e => console.error("å…³é—­AudioContextæ—¶å‡ºé”™:", e));
            }
            audioContext = null;

            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop()); // åœæ­¢éº¦å…‹é£éŸ³è½¨ (Stop microphone tracks)
                mediaStream = null; // æ¸…ç†MediaStreamå®ä¾‹ (Clean up MediaStream instance)
            }
            
            if (micButton) {
                micButton.textContent = 'ğŸ¤'; // é‡ç½®æŒ‰é’®æ–‡æœ¬ (Reset button text)
                micButton.style.backgroundColor = ''; // é‡ç½®æŒ‰é’®é¢œè‰² (Reset button color)
            }
        }

        // éº¦å…‹é£æŒ‰é’®äº‹ä»¶ç›‘å¬å™¨ (Microphone button event listener)
        micButton.addEventListener('click', () => {
            if (isRecording) {
                stopRecording();
            } else {
                startRecording();
            }
        });


        // --- Dify Chat å’Œ STT å…±åŒä½¿ç”¨çš„æ ·å¼ ---
        // (Styles used by both Dify Chat and STT)
        // (Styles used by both Dify Chat and STT)
        // ä¸ºæ¶ˆæ¯æ·»åŠ ä¸€äº›åŸºæœ¬æ ·å¼ï¼Œä»¥ä¾¿åŒºåˆ†ç”¨æˆ·å’ŒDify (Add basic styles for messages to differentiate user and Dify)
        const styleElement = document.createElement('style'); // Renamed to avoid conflict with potential Three.js 'style'
        styleElement.textContent = `
            .message {
                padding: 8px;
                margin-bottom: 5px;
                border-radius: 5px;
                max-width: 80%;
                word-wrap: break-word;
            }
            .user-message {
                background-color: #007bff;
                color: white;
                align-self: flex-end; /* ç”¨æˆ·æ¶ˆæ¯é å³ (User message align right) */
                margin-left: auto; /* ç”¨æˆ·æ¶ˆæ¯é å³ (User message align right) */
            }
            .dify-message {
                background-color: #f0f0f0;
                color: black;
                align-self: flex-start; /* Difyæ¶ˆæ¯é å·¦ (Dify message align left) */
                margin-right: auto; /* Difyæ¶ˆæ¯é å·¦ (Dify message align left) */
            }
            #chat-messages-display {
                display: flex;
                flex-direction: column;
            }
        `;
        document.head.appendChild(styleElement);
        // åˆå§‹æ¬¢è¿æ¶ˆæ¯çš„æ ·å¼å¯èƒ½ä¹Ÿéœ€è¦è°ƒæ•´ï¼Œä½†ä»¥ä¸Šæ ·å¼ä¸»è¦é’ˆå¯¹åŠ¨æ€æ·»åŠ çš„æ¶ˆæ¯
        // (Initial welcome message style might need adjustment, but above styles are for dynamically added messages)
        // å¯ä»¥è€ƒè™‘ç§»é™¤æˆ–è°ƒæ•´åˆå§‹çš„ <p>Welcome! ...</p> æ ‡ç­¾ï¼Œæˆ–è€…åœ¨JSä¸­åŠ¨æ€æ·»åŠ ç¬¬ä¸€æ¡æ¬¢è¿æ¶ˆæ¯
        // (Consider removing/adjusting initial <p>Welcome!...</p> or adding it dynamically in JS)


        // --- Three.js æ•°å­—äººæ˜¾ç¤ºé€»è¾‘ ---
        // (Three.js Digital Human Display Logic)
        const humanContainer = document.getElementById('digital-human-container'); // è·å–æ•°å­—äººå®¹å™¨ (Get digital human container)
        let scene, camera, renderer, modelMixer, clock; // å£°æ˜åœºæ™¯ã€ç›¸æœºã€æ¸²æŸ“å™¨ã€åŠ¨ç”»æ··åˆå™¨ã€æ—¶é’Ÿ (Declare scene, camera, renderer, animation mixer, clock)
        let loadedGltfModelScene = null; // ä¿å­˜åŠ è½½çš„GLTFæ¨¡å‹åœºæ™¯çš„å¼•ç”¨ (Store reference to the loaded GLTF model's scene)
        let characterMeshForMorphs = null; // ä¿å­˜ç”¨äºå£å‹åŒæ­¥çš„è§’è‰²ç½‘æ ¼å¼•ç”¨ (Store reference to the character mesh for morph targets)
        let mouthShapeIndex = undefined; // ä¿å­˜å˜´éƒ¨æ··åˆå½¢çŠ¶çš„ç´¢å¼• (Store index of the mouth blend shape)
        const MOUTH_SHAPE_NAME_CANDIDATES = ['mouthOpen', 'jawOpen', 'mouth', 'MouthOpen', 'JawOpen', 'mouth_open', 'jaw_open', 'jaw_Open', 'Mouth_Open']; // å¯èƒ½çš„å˜´éƒ¨æ··åˆå½¢çŠ¶åç§° (Possible mouth blend shape names)

        // åŠ¨ç”»ç®¡ç† (Animation Management)
        let animationActions = {}; // å­˜å‚¨æ‰€æœ‰åŠ¨ç”»åŠ¨ä½œ (Store all animation actions)
        let currentAction = null; // å½“å‰æ’­æ”¾çš„åŠ¨ç”»åŠ¨ä½œ (Current playing animation action)
        const DEFAULT_ANIMATION_NAME = 'Idle'; // é»˜è®¤åŠ¨ç”»åç§° (Default animation name)
        const LISTENING_ANIMATION_NAME = 'Survey'; // è†å¬/æ€è€ƒåŠ¨ç”»åç§° (Listening/Thinking animation name)
        // æ³¨æ„: 'Thinking' åŠ¨ç”»åœ¨ RobotExpressive.glb ä¸­å¯èƒ½ä¸å­˜åœ¨ï¼Œ'Survey' æ˜¯ä¸€ä¸ªè¾ƒå¥½çš„æ›¿ä»£å“
        // (Note: 'Thinking' animation might not exist in RobotExpressive.glb, 'Survey' is a good alternative)


        function initThreeJS() {
            // 1. åœºæ™¯è®¾ç½® (Scene Setup)
            scene = new THREE.Scene();
            scene.background = new THREE.Color(0x282c34); // è®¾ç½®åœºæ™¯èƒŒæ™¯è‰² (Set scene background color)

            // 2. ç›¸æœºè®¾ç½® (Camera Setup)
            if (!humanContainer) { 
                console.error("æ•°å­—äººå®¹å™¨ 'digital-human-container' æœªæ‰¾åˆ°ã€‚Three.js åˆå§‹åŒ–ä¸­æ­¢ã€‚");
                return;
            }
            camera = new THREE.PerspectiveCamera(75, humanContainer.clientWidth / humanContainer.clientHeight, 0.1, 1000);
            camera.position.set(0, 1.6, 3); 
            camera.lookAt(0, 1, 0); 

            // 3. æ¸²æŸ“å™¨è®¾ç½® (Renderer Setup)
            renderer = new THREE.WebGLRenderer({ antialias: true }); 
            renderer.setSize(humanContainer.clientWidth, humanContainer.clientHeight); 
            renderer.setPixelRatio(window.devicePixelRatio); 
            renderer.shadowMap.enabled = true; 
            renderer.outputEncoding = THREE.sRGBEncoding; 
            humanContainer.appendChild(renderer.domElement); 

            // 4. å…‰æºè®¾ç½® (Lighting Setup)
            const ambientLight = new THREE.AmbientLight(0xffffff, 0.6); 
            scene.add(ambientLight);
            const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8); 
            directionalLight.position.set(5, 10, 7.5); 
            directionalLight.castShadow = true; 
            directionalLight.shadow.mapSize.width = 1024;
            directionalLight.shadow.mapSize.height = 1024;
            directionalLight.shadow.camera.near = 0.5;
            directionalLight.shadow.camera.far = 50;
            scene.add(directionalLight);
            
            // 5. åŠ è½½3Dæ¨¡å‹ (Load 3D Model)
            const loader = new THREE.GLTFLoader();
            const modelUrl = 'https://cdn.jsdelivr.net/gh/mrdoob/three.js@r128/examples/models/gltf/RobotExpressive/RobotExpressive.glb';
            
            loader.load(modelUrl, 
                function (gltf) { 
                    loadedGltfModelScene = gltf.scene; 
                    loadedGltfModelScene.scale.set(0.7, 0.7, 0.7); 
                    loadedGltfModelScene.position.set(0, 0, 0);   
                    
                    loadedGltfModelScene.traverse(function (child) {
                        if (child.isMesh) {
                            child.castShadow = true;
                            child.receiveShadow = true;
                            if (child.morphTargetDictionary && Object.keys(child.morphTargetDictionary).length > 0) {
                                // å‡è®¾ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„å¸¦morph targetsçš„meshæ˜¯æˆ‘ä»¬è¦æ§åˆ¶çš„è§’è‰²mesh
                                // (Assume the first mesh found with morph targets is the character mesh we want to control)
                                if (!characterMeshForMorphs) {
                                    characterMeshForMorphs = child;
                                    console.log("æ‰¾åˆ°ç”¨äºå£å‹åŒæ­¥çš„è§’è‰²ç½‘æ ¼ (Found character mesh for lip sync):", characterMeshForMorphs.name);
                                    console.log("æ··åˆå½¢çŠ¶å­—å…¸ (Morph Target Dictionary):", characterMeshForMorphs.morphTargetDictionary);
                                    for (const nameCandidate of MOUTH_SHAPE_NAME_CANDIDATES) {
                                        if (characterMeshForMorphs.morphTargetDictionary[nameCandidate] !== undefined) {
                                            mouthShapeIndex = characterMeshForMorphs.morphTargetDictionary[nameCandidate];
                                            console.log(`æ‰¾åˆ°å˜´éƒ¨æ··åˆå½¢çŠ¶: '${nameCandidate}', ç´¢å¼•: ${mouthShapeIndex}`); 
                                            if (!characterMeshForMorphs.morphTargetInfluences) {
                                                characterMeshForMorphs.morphTargetInfluences = []; 
                                            }
                                            characterMeshForMorphs.morphTargetInfluences[mouthShapeIndex] = 0; 
                                            break; 
                                        }
                                    }
                                    if (mouthShapeIndex === undefined) {
                                        console.warn("è­¦å‘Š: æœªåœ¨æ­¤ç½‘æ ¼ä¸­æ‰¾åˆ°æ˜ç¡®çš„å˜´éƒ¨æ··åˆå½¢çŠ¶ã€‚å£å‹åŒæ­¥å¯èƒ½æ— æ³•å·¥ä½œã€‚"); 
                                    }
                                }
                            }
                        }
                    });
                    
                    scene.add(loadedGltfModelScene); 
                    console.log("3Dæ¨¡å‹åŠ è½½æˆåŠŸã€‚"); 

                    // åˆå§‹åŒ–åŠ¨ç”»æ··åˆå™¨å’ŒåŠ¨ä½œ (Initialize animation mixer and actions)
                    modelMixer = new THREE.AnimationMixer(loadedGltfModelScene);
                    animationActions = {}; // æ¸…ç©ºæ—§åŠ¨ä½œ (Clear old actions)
                    console.log("å¯ç”¨çš„åŠ¨ç”»ç‰‡æ®µ (Available animation clips):");
                    gltf.animations.forEach((clip) => {
                        const action = modelMixer.clipAction(clip);
                        animationActions[clip.name] = action;
                        console.log(`- ${clip.name}`);
                    });

                    // æ’­æ”¾é»˜è®¤åŠ¨ç”» (Play default animation)
                    let defaultAnim = animationActions[DEFAULT_ANIMATION_NAME] || animationActions[Object.keys(animationActions)[0]]; // Fallback to first anim
                    if (defaultAnim) {
                        currentAction = defaultAnim;
                        currentAction.play();
                        console.log(`æ’­æ”¾é»˜è®¤åŠ¨ç”»: ${currentAction.getClip().name}`);
                    } else {
                        console.warn("è­¦å‘Š: æœªæ‰¾åˆ°é»˜è®¤åŠ¨ç”»ï¼Œä¹Ÿæ— å…¶ä»–å¯ç”¨åŠ¨ç”»ã€‚");
                    }
                    clock = new THREE.Clock(); // åœ¨åŠ¨ç”»è®¾ç½®å®Œæˆååˆå§‹åŒ–æ—¶é’Ÿ (Initialize clock after animation setup)
                }, 
                undefined, 
                function (error) { 
                    console.error('3Dæ¨¡å‹åŠ è½½å¤±è´¥:', error); 
                    alert('æ— æ³•åŠ è½½3Dæ¨¡å‹ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ¨¡å‹ã€‚é”™è¯¯: ' + error.message); 
                    loadFallbackModel(); 
                }
            );

            // 6. çª—å£å¤§å°è°ƒæ•´å¤„ç† (Window Resize Handling)
            window.addEventListener('resize', onWindowResize, false);

            // 7. å¯åŠ¨æ¸²æŸ“å¾ªç¯ (Start Render Loop)
            animate();
        }

        function loadFallbackModel() {
            // åˆ›å»ºä¸€ä¸ªç®€å•çš„çƒä½“ä½œä¸ºå¤‡ç”¨æ¨¡å‹ (Create a simple sphere as a fallback model)
            const geometry = new THREE.SphereGeometry(0.8, 32, 32); // åŠå¾„0.8ç±³ (Radius 0.8m)
            const material = new THREE.MeshStandardMaterial({ color: 0x0077ff, roughness: 0.5, metalness: 0.1 });
            const placeholder = new THREE.Mesh(geometry, material);
            placeholder.position.set(0, 1, 0); // å°†çƒä½“æ”¾ç½®åœ¨(0,1,0)ä½ç½® (Place sphere at (0,1,0))
            placeholder.castShadow = true; // å¤‡ç”¨æ¨¡å‹ä¹ŸæŠ•å°„é˜´å½± (Fallback model also casts shadow)
            placeholder.receiveShadow = true;
            scene.add(placeholder);
            console.log("å·²åŠ è½½å¤‡ç”¨çƒä½“æ¨¡å‹ã€‚"); // Fallback sphere model loaded.
        }

        function onWindowResize() {
            // æ›´æ–°ç›¸æœºå®½é«˜æ¯” (Update camera aspect ratio)
            camera.aspect = humanContainer.clientWidth / humanContainer.clientHeight;
            camera.updateProjectionMatrix(); // åº”ç”¨æ›´æ”¹ (Apply changes)

            // æ›´æ–°æ¸²æŸ“å™¨å°ºå¯¸ (Update renderer size)
            renderer.setSize(humanContainer.clientWidth, humanContainer.clientHeight);
        }

        function animate() {
            requestAnimationFrame(animate); // è¯·æ±‚ä¸‹ä¸€å¸§åŠ¨ç”» (Request next animation frame)

            if (modelMixer) { // å¦‚æœå­˜åœ¨åŠ¨ç”»æ··åˆå™¨ (If animation mixer exists)
                modelMixer.update(clock.getDelta()); // æ›´æ–°åŠ¨ç”» (Update animation)
            }

            renderer.render(scene, camera); // æ¸²æŸ“åœºæ™¯ (Render the scene)
        }

        // DOMåŠ è½½å®Œæˆååˆå§‹åŒ–Three.jsåœºæ™¯ (Initialize Three.js scene after DOM is loaded)
        document.addEventListener('DOMContentLoaded', () => {
            if (humanContainer) {
                initThreeJS();
            } else {
                console.error("æœªæ‰¾åˆ°æ•°å­—äººå®¹å™¨ 'digital-human-container'ã€‚"); // Digital human container not found.
            }
        });

        // --- å°†éœ€è¦æµ‹è¯•çš„å‡½æ•°å’Œå˜é‡é™„åŠ åˆ°windowå¯¹è±¡ ---
        // (Attaching functions and variables to the window object for testing)
        window.floatTo16BitPCM = floatTo16BitPCM;
        window.addMessageToChat = addMessageToChat;
        window.playTextAsSpeech = playTextAsSpeech;
        window.sendMessageToDify = sendMessageToDify; // å‡è®¾sendMessageToDifyæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ã€å¯å…¨å±€è°ƒç”¨çš„å‡½æ•°
                                                    // (Assuming sendMessageToDify is a standalone, globally callable function)
                                                    // å¦‚æœå®ƒæ˜¯äº‹ä»¶ç›‘å¬å™¨çš„ä¸€éƒ¨åˆ†ï¼Œåˆ™éœ€è¦ä¸åŒçš„æµ‹è¯•ç­–ç•¥æˆ–é‡æ„
                                                    // (If it's part of an event listener, a different test strategy or refactoring would be needed)
        window.switchToAnimation = switchToAnimation;
        
        // æš´éœ² isRecording çŠ¶æ€å’ŒåŠ¨ç”»åç§°å¸¸é‡ï¼Œä»¥ä¾¿æµ‹è¯•å¯ä»¥æ£€æŸ¥/æ¨¡æ‹Ÿå®ƒä»¬
        // (Expose isRecording state and animation name constants so tests can check/mock them)
        // æ³¨æ„: ç›´æ¥æš´éœ² isRecording çŠ¶æ€ç”¨äºæµ‹è¯•å¯èƒ½è¡¨æ˜éœ€è¦æ›´ç²¾ç»†çš„çŠ¶æ€ç®¡ç†æˆ–äº‹ä»¶ã€‚
        // (Note: Directly exposing isRecording state for testing might indicate a need for more refined state management or events.)
        Object.defineProperty(window, 'isRecordingPublic', { // ä½¿ç”¨ getter/setter ä»¥é¿å…æ„å¤–çš„å…¨å±€ä¿®æ”¹
            get: function() { return isRecording; },
            set: function(value) { isRecording = value; } // å…è®¸æµ‹è¯•è®¾ç½®å®ƒ
        });
        window.LISTENING_ANIMATION_NAME_FOR_TEST = LISTENING_ANIMATION_NAME;
        window.DEFAULT_ANIMATION_NAME_FOR_TEST = DEFAULT_ANIMATION_NAME;


    </script>
    <script src="tests.js"></script> <!-- å•å…ƒæµ‹è¯•è„šæœ¬ (Unit Test Script) -->
    <button onclick="runUnitTests()" style="position:fixed; top:10px; right:10px; z-index:1000; padding: 8px 12px; background-color: #4CAF50; color: white; border: none; border-radius: 5px; cursor: pointer;">è¿è¡Œå•å…ƒæµ‹è¯•</button> <!-- (Run Unit Tests Button) -->
</body>
</html>
